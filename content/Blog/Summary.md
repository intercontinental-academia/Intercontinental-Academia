---
tags:
- Paris IAS
- ICA4
- Artificial Intelligence
- IEA de Paris
- Intelligence
- Behaviour
- Economics
- Cognitive Science
- Robots
- Ethics
published: true
pinned: false
post_title: 'Keeping Up with the ICA4: What''s happening each day at Paris IAS?'
date: 2021-10-19T22:00:00Z
authors:
- name: Atrina Oraee
images:
- "/snapseed-11.jpg"
- "/snapseed-9.jpg"
- "/snapseed-6.jpg"
- "/img_4949.jpg"
- "/img_4552.jpg"
- "/img_4211.jpg"
- "/snapseed-10.jpg"
- "/image.png"
- "/fullsizerender-2.jpg"
- "/snapseed-7.jpg"
youtube_video_id: ''
audio: []

---
Each day, the Fellows and their Mentors meet for **a closed 3-hour seminar**, during which two mentors launch the discussion with a presentation. Upon completion of the seminar, the Fellows then meet for 45 minutes to list the **key takeaways and ideas** that have emerged from the discussion, followed by a **collective brainstorming session**. This ensures that the output of collective intelligence is collected, formatted and capitalised.

The other half of the day is left free for participants to **reflect on the scientific discussions** in small groups. Such discussions are occasionally complemented by **lectures from the Chairs**. Finally, the Academia **travels to meet researchers** from some of the most prestigious universities in Paris. This includes a day at **Paris-Saclay**, a day at **Sorbonne University**, and finally, a day at the **ENS**. Upon completion of every morning's closed seminar at IAS, meetings and conferences are organised to encourage collective discussions and networking. The summary of each day is capitalised on the **ICA4 blog**. The following blogpost capitalises the key takeaways of each day’s sessions, as well as summarising the highlights.<!--more-->

# Day 1: **"The future needs wisdom”**

Chair: [Laura Candiotto](/fellows#candiotto "Laura Candiotto")

Scribe: [Massimiliano Di Luca](/fellows#di-luca "Massimiliano Di Luca")

**Perception, prediction, and pleasure: What can music teach us about neurocognition/intelligence?**

Presented by [_Robert Zatorre_](/mentors#zatorre "Robert Zatorre")

It was stated during the seminar that the brain represents the properties of the environment and guides behaviour through evaluation and reward. Aesthetic pleasure can be defined as the phylogenetically older system that is centred on the striatum.

Moreover, results of the relation between connectivity of the auditory cortex with the striatum and several behavioural results were presented (e.g. related to amusia, music-specific anhedonia). Dynamic causal modelling and predictive coding frameworks have been presented as possible explanations of the relationship between learning and reward in music. Predictions make the rewards evolve from a biological event to the expectation of the event.

Through the post-seminar collective discussions, the relevance of **affective experience** (pleasure and fear) in learning was emphasised. Discussions concluded with a rather open-ended question, leaving ICA4 Fellows wondering about whether or not AI should have a similar system for learning, and how should the reward and punishment be differentiated?" Maybe AI does not need to understand or experience human emotions; it just needs to behave like a human by capturing the features of a dataset that correctly describes the behaviour...

**High Energy Physics: Successes, Challenges and Magic**

Presented by [_Eliezer Rabinovici_](/about/ica4/#rabinovici "Eliezer Rabinovici")

It was discussed that observing natural phenomena can motivate scientific enquiry and drive us to understand the unknown. Moreover, equations are a way to increase predictability. However, a single, compact and reductionist explanation for all phenomena in the universe may not necessarily exist. The scientific method requires that results are reproducible. The correspondence principle requires that new theories can explain all phenomena for which a preceding theory was valid. To understand a phenomenon, one has to identify the relevant players and determine the correct explanation scale.

**In AI We Trust: Power, illusion and control of predictive algorithms**

Presented by [_Helga Nowotny_](http://helga-nowotny.eu "Helga Nowotny")

The session began with introducing the concept of singularity and defining it as a tipping point: a change of state that can lead to the collapse of a system. In an attempt to define Ethical AI, examples such as Transhumanism (ideas of transcending the limitations of a mortal body through information sharing) were discussed. Furthermore, the illusion that AI knows humans better than humans know themselves was elaborated, ultimately concluding by mentioning the existence of a possibility for human beings to both profit or suffer from an AI system depending on how it is applied.

"The future needs wisdom”: a urgent need to institutionalise context sensitivity, rather than creating a standardised system to control AI, was discussed and collectively developed. This lead to further debates regarding the concentration of technology advancement and its deteriorating impact on inequality. Thus, a global agreement is necessary to control AI, although it is currently almost impossible to obtain! Therefore, we should educate AI as a child of humanity that can grow to contribute to society. AI research is undertaken at such a massive scale that it requires global efforts which go beyond a single country. Scientists are paid by society, and their curiosity-led work should return to society as a whole...

# Day 2: **"In AI we trust"...or not!**

Scribe: [_Tahina Ralitera_](/fellows#ralitera "Tahina Ralitera")

Chair: [_Benjamin Guedj_](/fellows#guedj "Ben Guedj")

**Why Consciousness?**

Presented by [_Robert Aumann_](/mentors#aumann "Robert Aumann")

Essentially, the seminar was focused on the purpose which consciousness serves. Consciousness was defined as the ability to do the following:

* Perceive
* Feel (emotions)
* Think/intend
* Carry out intentions (volition)

Of all the above, perceiving, thinking/intending, and carrying out intentions may be done by machines. However, feeling and emotions belong exclusively to human beings. In such a context, it may be argued that the evolutionary function of consciousness is to enable the operation of emotions. This being said, we currently have no idea about how does consciousness work. Although considerable progress has been made in AI, Artificial Emotions (AE) has remained rather untouched.

**Myths and misunderstandings about responsibility for the unintended impact of AI**

Presented by [_Karen Yeung_](/mentors#yeung "Karen Yeung")

The talk mostly focused on responsibility for the unintended impact of Artificial Intelligence, based on the presenter's Council of Europe study. It was argued that Machine Learning's (ML) capacity to enable task automation and machine "autonomy" raise important questions about **responsibility**. Thus, responsibility-relevant attributes of ML were identified, for which an illustration is the data-driven profiling of individuals, and other ML applications, which may hold adverse impacts on human rights, on both individual and collective levels.  
While responsibility is important for human beings, who are considered as moral agents, to maintain peaceful social co-operation within the community, only a few studies have focused on tackling the fundamental role of responsibility for individuals, as well as the society.

The impacts produced by complex socio-technical systems using ML technologies have generated a range of concerns that fall under the heading of "algorithmic responsibility". While existing laws have an important role to play in ensuring the accountability of algorithmic systems, the implications of these technologies for their interference with human rights need to be studied further. This has been the primary focus of Karen Yeung's research.

In a nutshell, two dimensions of responsibility are required:

* **Historic or retrospective responsibility:** responsibility for conduct and events that occurred in the past
* **Prospective responsibility:** roles and tasks that look to the future

Finally, five common myths and misunderstandings concerning responsibility for the unintended adverse impacts of AI were identified:

* Need for effective and legitimate mechanisms to protect human rights from AI applications.
* Identifications of the appropriate responsibility model for allocating, distributing and preventing the various threats and risks.
* Responsibility of states to ensure that these policy choices are made in a transparent and democratic manner, in order to effectively protect human rights.
* Need for more interdisciplinary research
* Application of the fundamental principle of reciprocity so as not to allow those who develop and run our advanced digital technologies and systems to increase and exercise their power without responsibility.

**Data science and deep learning vs theory: two examples from economics and finance**

Presented by [_Raouf Boucekkine_](/about/ica4#boucekkine "Raouf Boucekkine")

The session included discussions on Data Science, Machine Learning (ML), and some relevant theories in the field of economics and finance that share common disciplines. Certain examples from macroeconomics, in which characteristics of the underlying mechanisms for complex systems are of great interest, were then discussed in more detail. In such context, a misunderstanding between different disciplines was highlighted: **the concept of equilibrium** is of great significance in mainstream macroeconomics, whereas this is not the case for statistical physics (e.g., the "equilibrium" bias outside the econ area, discussed by Bonneuil & Boucekkine (2020)). Finally, the use of various **methods** and approaches, such as DSGE (Dynamic Stochastic General Equilibrium), ABM (Agent-Based Modeling), and Neural Network-Based methods, in the field of macroeconomics were discussed.

# Day 3: **"What you do FOR  people, you do TO people, so do it WITH people"**

Scribe: [_André Fujita_](/fellows#fujita "Andre Fujita")

Chair: [_Philipp Kellmeyer_](/fellows#kellmeyer "Philipp Kellmeyer")

**Distributed Intelligence & Distributed Agency**

Presented by [_Saadi Lahlou_](/about/ica4#lahlou "Saadi Lahlou")

We want intelligence to perform relevantly adapted actions that change the situation in which we are for the better. To design intelligence, we must first understand **the nature of the actual activity**. In this sense, behaviour was defined as what people do, seen from the outside. In other words, behaviour remains an external description of objective phenomena. This is while activity is how people subjectively perceive their action and how they see it from their own perspective.

**Installations consist of components that simultaneously support and control**. In other words, they are specific, local, and societal settings where humans are expected to behave predictably, e.g., airport, metro, cash machine, etc. **Installations consist of three layers: affordances of the physical environment embodied competencies, and social regulations. Intelligence is distributed over three layers.**

The question now is: why do we have these installations? Because **installations channel many of our behaviours** and consequently make us very efficient, although our short memory and cognitive processing are very limited compared to animals. Installations are **redundant**, and redundancy produces resilience and learning.

Moreover, certain questions on designing trade-off issues were raised: which agency for whom? AI agent? what kind of competence for the AI? What affordances? What rules? What degree of awareness? To whom does the agent report? How is it evaluated? He also added the “privacy dilemma.” In other words, for better service, one must disclose information. Is there an “agency dilemma”? Can we make it explicit? Because the agency is distributed, so the responsibility is shared. It means that we now have the “many hands" problem. Thus, credentials for AI were suggested, which include **values** (what does it try to optimise), **ownership** (who takes responsibility for its actions), **principles of action** (rules, algorithms, domain of awareness and action), **track record** (list of transactions executed, includes initial training).

To conclude, ICA4 Fellows were left with some questions as food for thought. For each activity, do we want to augment existing agents with more agency? If so, which ones? Humans? Material objects? Social system? New agents? Who learns what? What values do we want to foster? What do we want AI for must be addressed for each activity, starting from the activity and discussed?

**Perspective on Artificial Intelligence research from studies on Agency, self-recognition and social cognition in animals**

Presented by [_William Hopkins_](/mentors#hopkins "William Hopkins")

The session began by discussing humans constructed concepts to reflect intellectual abilities in various domains of cognitive functions. In this sense, we use tests like the WAIS or Stanford Binet to quantify and scale performance to standards for specific age classes. These tests rely heavily on language. There are many approaches to developing fair tests of cognition between species with different sensory and motor capabilities. It began with Darwin and Furness. Then, George Romanes (1884) focused on animal intelligence and later on, Kohler (1925) on insight learning. Within the same field, Robert Yerkes (1916) worked on “The mental life of monkeys and apes: a study of ideational behavior”. Yerkes later developed the IQ test used by the army in WW1 (army alpha test).

Upon drawing on some of the literature, several videos were played, in which apes carries out various tasks including retrieving a peanut in the bottom of a tube followed by a video from an ape imitating a human, and so on. Several animals passed the mark test. E.g., magpies with a yellow stick in their neck can identify it and try to remove using the mirror. Cortical parcellation of chimpanzee brain - compared to humans, the ones that passed the test show differences in some cortices. Grey matter differences between MSR+ vs. MSR-. They also analysed the anterior cingulate since such neurones are rather long and connect the anterior cingulate with the insula.

Moreover, results from studies that showed that human children outperformed apes on the social, but not physical, cognition tasks were presented and discussed. Much like the research in AI, Most early comparative studies of cognition and intelligence were strongly rooted in associative learning theory. However, associative or operant theories of learning were and are notoriously anti-cognitive. In the 1960s, there were attempts to reach apes alternative communication systems. The goal of the ape language studies was to determine whether language is uniquely human. The answer depends on how we define language.

However, is it language? There is very little evidence for declarative production (e.g., turn on the TV, give me an onion) in communication signals by primates and other animals. The other question is: are social stimuli rewarding? For chimpanzees, yes. Experiment: touch one button to see other chimpanzees or another button to see random animals. The chimp chose to press the button to see other chimps. Thus, the role of reward guides the learning and behaviour of animals. Although animal cognition is often used to explain animal behaviour, most can be explained by an associative learning mechanism.

**AI and Robots for Future: The Moon Shot Project**

Presented by [_Toshio Fukuda_](/mentors#fukuda "Toshio Fukuda")

Robots are avatars that pop up to help when humans need them. There is an information and physical interaction between robots and humans. Toshio showed several multi-scale robots, e.g., monkey-type robots, multi-locomotion, intelligent cane, etc. One of these robots is the Brachiator I-III. Brachiation is a form of long-armed ape locomotion. It uses dynamics of the pendulum, under-actuated mechanical system, variable constraint system, machine learning, AI, reinforce learning, soft computing (fuzzy, genetic algorithm). Regarding multi-locomotion types, in many cases, one creature has multiple types of locomotion in order to improve its mobility. The motivation of their study is to develop a robot mechanism and a control architecture that can achieve multiple locomotions. Hybrid computational intelligence, i.e., AI and brain interface were also commented upon by the speaker while showing a series of related videos. An example of such videos illustrated the Boston dynamics atlas and others: three robots dancing and jumping which was quite impressive!

Moreover, AI+Robot+IoT (Internet of Things), the use of robots in mega-trend (energy, urbanisation, food, ageing, global warming, robot, and AI) were discussed. This was followed by further discussions on autonomous cars, which may be safer than human drivers, mixed reality, the Eve project (a transparent body that simulates the human body), cyborg technology (fusion of robot and animal), and multiple robots (communication among robots).

Finally, the Moonshot project was revealed: **a society where humans and robots live together in 2050!**

# Day 4: **A visit to The University of Paris-Saclay**

Scribe: [_Alex Cayco Gajic_](/fellows#cayco-gajic "Alex Cayco-Gajic")

Chair: [_Diego Frassinelli_](/fellows#frassinelli "Diego Frassinelli")

The scientific sessions at Saclay included two thought-provoking talks by [Xiao-Jing Wang](/mentors/#wang "Xiao-Jing Wang") and [Jay McClelland](/mentors/#mcclelland "Jay McClelland"), both of which touched upon **the principles underlying cognitive behaviours**, as well as **the difference between human and machine intelligence**. These were followed by a half-day symposium on AI organized in conjunction with the [Ecole Normale Superieure de Paris-Saclay](http://ens-paris-saclay.fr/en "Paris Saclay") which hosted us through the day.

First, **Wang** discussed his efforts to understand the **computational principles underlying cognition**. Deep neural networks, despite their recent success, differ from human cognition because they have no internal mental life - instead, they act as complex, nonlinear input-output functions. In humans, the prefrontal cortex (PFC) is known to be crucial for cognitive functions such as working memory, decision making, and executive function. An early avenue of this research involved understanding how persistent neural activity may underlie working memory by sustaining stimulus information in the brain after the sensory cue has disappeared. Such persistence is linked to recurrent connectivity, which is lacking in most deep networks. Wang described his previous research using spiking networks and tools from dynamical systems to understand the attractor dynamics behind this form of memory. In the second half of the talk, he showcased his more recent work which uses recurrent neural networks (RNNs) as a form of a model organism to probe how the PFC may perform multiple cognitive tasks simultaneously. These RNNs can then be used to address questions such as whether the PFC encodes cognitive building blocks in a compositional manner, similar to the psychological concepts of schema.

Following this talk, **McClelland** highlighted **a different distinction between human intelligence and AI**. While the latter (in particular machine learning algorithms) learns from statistics on large-scale input data, humans learn to learn from explanations structured by culturally invented systems. Indeed, humans fail to perform in systematic ways, which we would expect if the structure were built into our cognitive functionality. But, McClelland points out that simply building in structure, as proposed by the pioneers of GOFAI, limits flexibility. This structure, McClelland argued, is built by **culture**. For example, he described a classic study by Scribner and Cole in 1973 which showed that non-Western cultures often lack a concept of absolute number and tend to classify objects based on concrete situations rather than abstract category membership. These authors proposed that Western education creates a context in which certain abstract relational concepts are learned, consistent with McClelland’s later work correlating sudoku puzzle performance to mathematical education level. McClelland closed by reiterating that AI learns by examples but humans learn by explanations and that his explanation-based learning (rather than built-in structure) may underlie our propensity for one-shot learning.

Upon completion of the talks by ICA4 Mentors, Paris-Saclay hosted **a half-day event with multiple workshops** in which **ICA4 mentors and Paris-Saclay researchers discussed major advances and issues surrounding AI**. Dehaene also presented a series of fMRI, MEG, and behavioural evidence that **humans use symbolic and recursive strategies on prediction tasks with complex sequences**, as compared with monkeys which seem to use a picture-based strategy. In a session focusing on AI and ethics, Paola Tubaro revealed the hidden human workers who provide the hand-labelled training data for products like Siri. Because companies need cheaper work in the same language this tends to reproduce historic colonial patterns.

Finally, the intellectually intense day came to an end with a talk in which [**Zaven Paré**](/mentors/#pare "Zaven Pare") discussed his **artistic works based on electronic marionettes** and his collaborations with robotics specialists in Japan. Paré’s conception of automaton-centred theatre enchants audiences while challenging our tendency towards **anthropomorphisation**. This raises important questions regarding **how we will interact with AI algorithms and intelligent robotics in the decades to come...**

# Day 5: "**an Argus with billions of eyes and ears, capacity for action, and infinite memory!"**

Scribe: [_Oksana Stalnov_](/fellows/#stalnov "Oksana Stalnov")

Chair: [_Evandro Cunha_](/fellows/#cunha "Evandro Cunha")

While impossible to precisely predict, the convoluted future of AI may be presented in terms of **some major challenges it will inevitably face within the upcoming decades**. The ICA4 Mentors shared their views and thoughts on this fascinating, and intellectually challenging, subject over a roundtable discussion in the halls of Paris IAS! _Find out more about their thoughts and **perceived challenges for AI **_[**_here_**](https://www.intercontinental-academia.org/blog/Some%20Challenges%20of%20AI/ "Challenges of AI")_._