---
tags:
- Paris IAS
- ICA4
- Artificial Intelligence
- IEA de Paris
- Intelligence
- Behaviour
- Economics
- Cognitive Science
- Robots
- Ethics
published: false
pinned: false
post_title: 'Keeping Up with the ICA4: What''s happening each day at Paris IAS?'
date: 2021-10-19T22:00:00Z
authors:
- name: Atrina Oraee
images:
- "/snapseed-11.jpg"
- "/snapseed-57.jpg"
- "/snapseed-6.jpg"
- "/screenshot-2021-10-27-at-06-49-27.png"
- "/snapseed-38.jpg"
- "/snapseed-40.jpg"
- "/snapseed-9.jpg"
- "/img_4949.jpg"
- "/snapseed-49.jpg"
- "/snapseed-58.jpg"
- "/img_4552.jpg"
- "/img_5346.jpg"
- "/snapseed-51.jpg"
- "/snapseed-39.jpg"
- "/img_4211.jpg"
- "/snapseed-29.jpg"
- "/snapseed-54.jpg"
- "/screenshot-2021-10-27-at-06-48-00.png"
- "/snapseed-10.jpg"
- "/snapseed-47.jpg"
- "/image.png"
- "/snapseed-32.jpg"
- "/fullsizerender-2.jpg"
- "/snapseed-28.jpg"
- "/snapseed-7.jpg"
youtube_video_id: ''
audio: []

---
Each day, the Fellows and their Mentors meet for **a closed 3-hour seminar**, during which two mentors launch the discussion with a presentation. Upon completion of the seminar, the Fellows then meet for 45 minutes to list the **key takeaways and ideas** that have emerged from the discussion, followed by a **collective brainstorming session**. This ensures that the output of collective intelligence is collected, formatted and capitalised.

The other half of the day is left free for participants to **reflect on the scientific discussions** in small groups. Such discussions are occasionally complemented by **lectures from the Chairs**. Finally, the Academia **travels to meet researchers** from some of the most prestigious universities in Paris. This includes a day at **Paris-Saclay**, a day at **Sorbonne University**, and finally, a day at the **ENS**. Upon completion of every morning's closed seminar at IAS, meetings and conferences are organised to encourage collective discussions and networking. The summary of each day is capitalised on the **ICA4 blog**. The following blogpost capitalises the key takeaways of each day’s sessions, as well as summarising the highlights.<!--more-->

# Day 1: **"The future needs wisdom”**

Chair: [_Laura Candiotto_](/fellows#candiotto "Laura Candiotto")

Scribe: [_Massimiliano Di Luca_](/fellows#di-luca "Massimiliano Di Luca")

**Perception, prediction, and pleasure: What can music teach us about neurocognition/intelligence?**

Presented by [**_Robert Zatorre_**](/mentors#zatorre "Robert Zatorre")

It was stated during the seminar that the brain represents the properties of the environment and guides behaviour through evaluation and reward. Aesthetic pleasure can be defined as the phylogenetically older system that is centred on the striatum.

Moreover, results of the relation between connectivity of the auditory cortex with the striatum and several behavioural results were presented (e.g. related to amusia, music-specific anhedonia). Dynamic causal modelling and predictive coding frameworks have been presented as possible explanations of the relationship between learning and reward in music. Predictions make the rewards evolve from a biological event to the expectation of the event.

Through the post-seminar collective discussions, the relevance of **affective experience** (pleasure and fear) in learning was emphasised. Discussions concluded with a rather open-ended question, leaving ICA4 Fellows wondering about whether or not AI should have a similar system for learning, and how should the reward and punishment be differentiated?" Maybe AI does not need to understand or experience human emotions; it just needs to behave like a human by capturing the features of a dataset that correctly describes the behaviour...

**High Energy Physics: Successes, Challenges and Magic**

Presented by [**_Eliezer Rabinovici_**](/about/ica4/#rabinovici "Eliezer Rabinovici")

It was discussed that observing natural phenomena can motivate scientific enquiry and drive us to understand the unknown. Moreover, equations are a way to increase predictability. However, a single, compact and reductionist explanation for all phenomena in the universe may not necessarily exist. The scientific method requires that results are reproducible. The correspondence principle requires that new theories can explain all phenomena for which a preceding theory was valid. To understand a phenomenon, one has to identify the relevant players and determine the correct explanation scale.

**In AI We Trust: Power, illusion and control of predictive algorithms**

Presented by [**_Helga Nowotny_**](http://helga-nowotny.eu "Helga Nowotny")

The session began with introducing the concept of singularity and defining it as a tipping point: a change of state that can lead to the collapse of a system. In an attempt to define Ethical AI, examples such as Transhumanism (ideas of transcending the limitations of a mortal body through information sharing) were discussed. Furthermore, the illusion that AI knows humans better than humans know themselves was elaborated, ultimately concluding by mentioning the existence of a possibility for human beings to both profit or suffer from an AI system depending on how it is applied.

"The future needs wisdom”: a urgent need to institutionalise context sensitivity, rather than creating a standardised system to control AI, was discussed and collectively developed. This lead to further debates regarding the concentration of technology advancement and its deteriorating impact on inequality. Thus, a global agreement is necessary to control AI, although it is currently almost impossible to obtain! Therefore, we should educate AI as a child of humanity that can grow to contribute to society. AI research is undertaken at such a massive scale that it requires global efforts which go beyond a single country. Scientists are paid by society, and their curiosity-led work should return to society as a whole...

# Day 2: **"In AI we trust"...or not!**

Scribe: [_Tahina Ralitera_](/fellows#ralitera "Tahina Ralitera")

Chair: [_Benjamin Guedj_](/fellows#guedj "Ben Guedj")

**Why Consciousness?**

Presented by [**_Robert Aumann_**](/mentors#aumann "Robert Aumann")

Essentially, the seminar was focused on the purpose which consciousness serves. Consciousness was defined as the ability to do the following:

* Perceive
* Feel (emotions)
* Think/intend
* Carry out intentions (volition)

Of all the above, perceiving, thinking/intending, and carrying out intentions may be done by machines. However, feeling and emotions belong exclusively to human beings. In such a context, it may be argued that the evolutionary function of consciousness is to enable the operation of emotions. This being said, we currently have no idea about how does consciousness work. Although considerable progress has been made in AI, Artificial Emotions (AE) has remained rather untouched.

**Myths and misunderstandings about responsibility for the unintended impact of AI**

Presented by [**_Karen Yeung_**](/mentors#yeung "Karen Yeung")

The talk mostly focused on responsibility for the unintended impact of Artificial Intelligence, based on the presenter's Council of Europe study. It was argued that Machine Learning's (ML) capacity to enable task automation and machine "autonomy" raise important questions about **responsibility**. Thus, responsibility-relevant attributes of ML were identified, for which an illustration is the data-driven profiling of individuals, and other ML applications, which may hold adverse impacts on human rights, on both individual and collective levels.  
While responsibility is important for human beings, who are considered as moral agents, to maintain peaceful social co-operation within the community, only a few studies have focused on tackling the fundamental role of responsibility for individuals, as well as the society.

The impacts produced by complex socio-technical systems using ML technologies have generated a range of concerns that fall under the heading of "algorithmic responsibility". While existing laws have an important role to play in ensuring the accountability of algorithmic systems, the implications of these technologies for their interference with human rights need to be studied further. This has been the primary focus of Karen Yeung's research.

In a nutshell, two dimensions of responsibility are required:

* **Historic or retrospective responsibility:** responsibility for conduct and events that occurred in the past
* **Prospective responsibility:** roles and tasks that look to the future

Finally, five common myths and misunderstandings concerning responsibility for the unintended adverse impacts of AI were identified:

* Need for effective and legitimate mechanisms to protect human rights from AI applications.
* Identifications of the appropriate responsibility model for allocating, distributing and preventing the various threats and risks.
* Responsibility of states to ensure that these policy choices are made in a transparent and democratic manner, in order to effectively protect human rights.
* Need for more interdisciplinary research
* Application of the fundamental principle of reciprocity so as not to allow those who develop and run our advanced digital technologies and systems to increase and exercise their power without responsibility.

**Data science and deep learning vs theory: two examples from economics and finance**

Presented by [**_Raouf Boucekkine_**](/about/ica4#boucekkine "Raouf Boucekkine")

The session included discussions on Data Science, Machine Learning (ML), and some relevant theories in the field of economics and finance that share common disciplines. Certain examples from macroeconomics, in which characteristics of the underlying mechanisms for complex systems are of great interest, were then discussed in more detail. In such context, a misunderstanding between different disciplines was highlighted: **the concept of equilibrium** is of great significance in mainstream macroeconomics, whereas this is not the case for statistical physics (e.g., the "equilibrium" bias outside the econ area, discussed by Bonneuil & Boucekkine (2020)). Finally, the use of various **methods** and approaches, such as DSGE (Dynamic Stochastic General Equilibrium), ABM (Agent-Based Modeling), and Neural Network-Based methods, in the field of macroeconomics were discussed.

# Day 3: **"What you do FOR  people, you do TO people, so do it WITH people"**

Scribe: [_André Fujita_](/fellows#fujita "Andre Fujita")

Chair: [_Philipp Kellmeyer_](/fellows#kellmeyer "Philipp Kellmeyer")

**Distributed Intelligence & Distributed Agency**

Presented by [**_Saadi Lahlou_**](/about/ica4#lahlou "Saadi Lahlou")

We want intelligence to perform relevantly adapted actions that change the situation in which we are for the better. To design intelligence, we must first understand **the nature of the actual activity**. In this sense, behaviour was defined as what people do, seen from the outside. In other words, behaviour remains an external description of objective phenomena. This is while activity is how people subjectively perceive their action and how they see it from their own perspective.

**Installations consist of components that simultaneously support and control**. In other words, they are specific, local, and societal settings where humans are expected to behave predictably, e.g., airport, metro, cash machine, etc. **Installations consist of three layers: affordances of the physical environment embodied competencies, and social regulations. Intelligence is distributed over three layers.**

The question now is: why do we have these installations? Because **installations channel many of our behaviours** and consequently make us very efficient, although our short memory and cognitive processing are very limited compared to animals. Installations are **redundant**, and redundancy produces resilience and learning.

Moreover, certain questions on designing trade-off issues were raised: which agency for whom? AI agent? what kind of competence for the AI? What affordances? What rules? What degree of awareness? To whom does the agent report? How is it evaluated? He also added the “privacy dilemma.” In other words, for better service, one must disclose information. Is there an “agency dilemma”? Can we make it explicit? Because the agency is distributed, so the responsibility is shared. It means that we now have the “many hands" problem. Thus, credentials for AI were suggested, which include **values** (what does it try to optimise), **ownership** (who takes responsibility for its actions), **principles of action** (rules, algorithms, domain of awareness and action), **track record** (list of transactions executed, includes initial training).

To conclude, ICA4 Fellows were left with some questions as food for thought. For each activity, do we want to augment existing agents with more agency? If so, which ones? Humans? Material objects? Social system? New agents? Who learns what? What values do we want to foster? What do we want AI for must be addressed for each activity, starting from the activity and discussed?

**Perspective on Artificial Intelligence research from studies on Agency, self-recognition and social cognition in animals**

Presented by [**_William Hopkins_**](/mentors#hopkins "William Hopkins")

The session began by discussing humans constructed concepts to reflect intellectual abilities in various domains of cognitive functions. In this sense, we use tests like the WAIS or Stanford Binet to quantify and scale performance to standards for specific age classes. These tests rely heavily on language. There are many approaches to developing fair tests of cognition between species with different sensory and motor capabilities. It began with Darwin and Furness. Then, George Romanes (1884) focused on animal intelligence and later on, Kohler (1925) on insight learning. Within the same field, Robert Yerkes (1916) worked on “The mental life of monkeys and apes: a study of ideational behavior”. Yerkes later developed the IQ test used by the army in WW1 (army alpha test).

Upon drawing on some of the literature, several videos were played, in which apes carries out various tasks including retrieving a peanut in the bottom of a tube followed by a video from an ape imitating a human, and so on. Several animals passed the mark test. E.g., magpies with a yellow stick in their neck can identify it and try to remove using the mirror. Cortical parcellation of chimpanzee brain - compared to humans, the ones that passed the test show differences in some cortices. Grey matter differences between MSR+ vs. MSR-. They also analysed the anterior cingulate since such neurones are rather long and connect the anterior cingulate with the insula.

Moreover, results from studies that showed that human children outperformed apes on the social, but not physical, cognition tasks were presented and discussed. Much like the research in AI, Most early comparative studies of cognition and intelligence were strongly rooted in associative learning theory. However, associative or operant theories of learning were and are notoriously anti-cognitive. In the 1960s, there were attempts to reach apes alternative communication systems. The goal of the ape language studies was to determine whether language is uniquely human. The answer depends on how we define language.

However, is it language? There is very little evidence for declarative production (e.g., turn on the TV, give me an onion) in communication signals by primates and other animals. The other question is: are social stimuli rewarding? For chimpanzees, yes. Experiment: touch one button to see other chimpanzees or another button to see random animals. The chimp chose to press the button to see other chimps. Thus, the role of reward guides the learning and behaviour of animals. Although animal cognition is often used to explain animal behaviour, most can be explained by an associative learning mechanism.

**AI and Robots for Future: The Moon Shot Project**

Presented by [_Toshio Fukuda_](/mentors#fukuda "Toshio Fukuda")

Robots are avatars that pop up to help when humans need them. There is an information and physical interaction between robots and humans. Toshio showed several multi-scale robots, e.g., monkey-type robots, multi-locomotion, intelligent cane, etc. One of these robots is the Brachiator I-III. Brachiation is a form of long-armed ape locomotion. It uses dynamics of the pendulum, under-actuated mechanical system, variable constraint system, machine learning, AI, reinforce learning, soft computing (fuzzy, genetic algorithm). Regarding multi-locomotion types, in many cases, one creature has multiple types of locomotion in order to improve its mobility. The motivation of their study is to develop a robot mechanism and a control architecture that can achieve multiple locomotions. Hybrid computational intelligence, i.e., AI and brain interface were also commented upon by the speaker while showing a series of related videos. An example of such videos illustrated the Boston dynamics atlas and others: three robots dancing and jumping which was quite impressive!

Moreover, AI+Robot+IoT (Internet of Things), the use of robots in mega-trend (energy, urbanisation, food, ageing, global warming, robot, and AI) were discussed. This was followed by further discussions on autonomous cars, which may be safer than human drivers, mixed reality, the Eve project (a transparent body that simulates the human body), cyborg technology (fusion of robot and animal), and multiple robots (communication among robots).

Finally, the Moonshot project was revealed: **a society where humans and robots live together in 2050!**

# Day 4: **A visit to The University of Paris-Saclay**

Scribe: [_Alex Cayco Gajic_](/fellows#cayco-gajic "Alex Cayco-Gajic")

Chair: [_Diego Frassinelli_](/fellows#frassinelli "Diego Frassinelli")

The scientific sessions at Saclay included two thought-provoking talks by [**_Xiao-Jing Wang_**](/mentors/#wang "Xiao-Jing Wang") and [**_Jay McClelland_**](/mentors/#mcclelland "Jay McClelland"), both of which touched upon **the principles underlying cognitive behaviours**, as well as **the difference between human and machine intelligence**. These were followed by a half-day symposium on AI organized in conjunction with the [**Ecole Normale Superieure de Paris-Saclay**](http://ens-paris-saclay.fr/en "Paris Saclay") which hosted us through the day.

First, **Wang** discussed his efforts to understand the **computational principles underlying cognition**. Deep neural networks, despite their recent success, differ from human cognition because they have no internal mental life - instead, they act as complex, nonlinear input-output functions. In humans, the prefrontal cortex (PFC) is known to be crucial for cognitive functions such as working memory, decision making, and executive function. An early avenue of this research involved understanding how persistent neural activity may underlie working memory by sustaining stimulus information in the brain after the sensory cue has disappeared. Such persistence is linked to recurrent connectivity, which is lacking in most deep networks. Wang described his previous research using spiking networks and tools from dynamical systems to understand the attractor dynamics behind this form of memory. In the second half of the talk, he showcased his more recent work which uses recurrent neural networks (RNNs) as a form of a model organism to probe how the PFC may perform multiple cognitive tasks simultaneously. These RNNs can then be used to address questions such as whether the PFC encodes cognitive building blocks in a compositional manner, similar to the psychological concepts of schema.

Following this talk, **McClelland** highlighted **a different distinction between human intelligence and AI**. While the latter (in particular machine learning algorithms) learns from statistics on large-scale input data, humans learn to learn from explanations structured by culturally invented systems. Indeed, humans fail to perform in systematic ways, which we would expect if the structure were built into our cognitive functionality. But, McClelland points out that simply building in structure, as proposed by the pioneers of GOFAI, limits flexibility. This structure, McClelland argued, is built by **culture**. For example, he described a classic study by Scribner and Cole in 1973 which showed that non-Western cultures often lack a concept of absolute number and tend to classify objects based on concrete situations rather than abstract category membership. These authors proposed that Western education creates a context in which certain abstract relational concepts are learned, consistent with McClelland’s later work correlating sudoku puzzle performance to mathematical education level. McClelland closed by reiterating that AI learns by examples but humans learn by explanations and that his explanation-based learning (rather than built-in structure) may underlie our propensity for one-shot learning.

Upon completion of the talks by ICA4 Mentors, Paris-Saclay hosted **a half-day event with multiple workshops** in which **ICA4 mentors and Paris-Saclay researchers discussed major advances and issues surrounding AI**. Dehaene also presented a series of fMRI, MEG, and behavioural evidence that **humans use symbolic and recursive strategies on prediction tasks with complex sequences**, as compared with monkeys which seem to use a picture-based strategy. In a session focusing on AI and ethics, Paola Tubaro revealed the hidden human workers who provide the hand-labelled training data for products like Siri. Because companies need cheaper work in the same language this tends to reproduce historic colonial patterns.

Finally, the intellectually intense day came to an end with a talk in which [**_Zaven Paré_**](/mentors/#pare "Zaven Pare") discussed his **artistic works based on electronic marionettes** and his collaborations with robotics specialists in Japan. Paré’s conception of automaton-centred theatre enchants audiences while challenging our tendency towards **anthropomorphisation**. This raises important questions regarding **how we will interact with AI algorithms and intelligent robotics in the decades to come...**

# Day 5: "**an Argus with billions of eyes and ears, capacity for action, and infinite memory!"**

Scribe: [_Oksana Stalnov_](/fellows/#stalnov "Oksana Stalnov")

Chair: [_Evandro Cunha_](/fellows/#cunha "Evandro Cunha")

While impossible to precisely predict, the convoluted future of AI may be presented in terms of **some major challenges it will inevitably face within the upcoming decades**. The ICA4 Mentors shared their views and thoughts on this fascinating, and intellectually challenging, subject over a roundtable discussion in the halls of Paris IAS! _Find out more about their thoughts and perceived challenges for AI_ [**_here_**](https://www.intercontinental-academia.org/blog/Some%20Challenges%20of%20AI/ "Challenges of AI")_._

# Day 6: **A trip to Sorbonne Center for Artificial Intelligence - SCAI**

Scribe: [_Henry Taylor_](/fellows#taylor "Henry Taylor")

Chair: [_Ithai Rabinowitch_](/fellows#rabinowitch "Thai Rabinowitch")

**Computational indeterminacy: what is your computer doing?**

While at [**SCAI**](https://scai.sorbonne-universite.fr "SCAI"), The [ICA4 Fellows](/fellows "Fellows") plunged into **the** **philosophy of computation** with two [ICA4 mentors](/mentors "Mentors"), who are philosophers of cognitive science ([**_Jack Copeland _**](/mentors#copeland)and [**_Oron Shagrir_**](/mentors#shagrir "Oron Shagrir")). We focussed on the notion of **indeterminacy in computation**. Perhaps it's necessary to describe what computational indeterminacy is, and then look at its applications and philosophical questions.

**_So, what is computational indeterminacy anyway?_**

Suppose you have a computer, carrying out all sorts of tasks. Electrical signals are pulsing through it, flowing across its circuits, inputs flowing in, outputs flowing out. Now, we can ask the question: **what is the computer doing?** Or, more precisely: **which computations is it actually carrying out?**

This may seem easy to answer: you simply look at the machine and find out which computations it’s carrying out, surely? Well, no. Here is where the issue of computational indeterminacy comes in, and to understand it we’ll need a tiny bit of logic.

Computers make use of what are called _logic gates_, or simply: gates. Each gate takes a certain set of inputs, then (based on the rules that govern the gate), it gives a certain output. So let’s take a really simple gate, with two inputs, and one output. Inputs and outputs are in the form of electrical pulses (so it can take in up to two electrical pulses as inputs, and give up to one electrical pulse as an output). Furthermore, the _absence_ of a pulse can also be an output or an input as well.

Let’s say the gate follows this rule:

**GATE-RULE:**

_IF I RECEIVE TWO ELECTRICAL PULSES, THEN I WILL OUTPUT ONE PULSE. IF I DO NOT RECEIVE TWO ELECTRICAL PULSES, I WILL NOT OUTPUT A PULSE._

More simply, **the gate’s job is to only output a pulse if it receives two pulses as input. Otherwise, it stays totally silent.** So far so good...right?

Now the core question: **what is the gate computing?** Someone with a teeny bit of undergraduate logic may think the answer is obvious: it’s computing AND. AND one of the most basic logical functions, which is characterised by the following rule:

**AND-RULE:**

_IF I RECEIVE TWO INPUTS OF ‘TRUE’ (OR ‘1’) THEN I WILL OUTPUT ‘TRUE’ (OR ‘1’). OTHERWISE, I WILL OUTPUT ‘FALSE’ (OR ‘0’)._

It initially looks pretty clear that the gate is computing AND. If you compare the AND-RULE to the GATE-RULE, they look pretty much the same. After all, to get the AND-RULE from the GATE-RULE, you just need to substitute an electrical pulse for ‘TRUE’ or ‘1’, and the absence of a pulse for ‘FALSE’ or ‘0’.

So what’s the problem? It stems from the way we map pulses to ‘TRUE’ (or ‘1’) and non-pulses to ‘FALSE’ (or ‘0’). If we interpret a pulse as TRUE, and a non-pulse as FALSE, then the gate will indeed compute the function AND (characterised by the AND-RULE, above). But that’s just a choice we made, to map pulses to TRUE and non-pulses to FALSE in these ways.

What if we did it the other way round? What if we switched the mappings round? What if we interpret a pulse as FALSE, and non-pulse as TRUE? What then? The gate will still output a pulse when (and only when) it receives two pulses. But now (since we’ve switched the mappings around), what it’s doing is outputting a ‘FALSE’ only when it receives two ‘FALSE’ inputs. In fact, what it’s now doing (as any quick-witted first year logic student will tell you) is computing OR. That is, it is computing this:

**OR-RULE**

_IF I RECEIVE TWO INPUTS OF ‘FALSE’ (OR ‘0’) THEN I WILL OUTPUT ‘FALSE’ (OR ‘0’). OTHERWISE, I WILL OUTPUT ‘TRUE’ (OR ‘1’)._

So this is pretty weird. **Whether the gate is computing AND or OR is a matter of how we interpret it.** It’s a matter of how we map electrical pulses to TRUE/1, or FALSE/0. There’s no correct answer to which computation the gate itself is doing, that’s a matter of how we interpret it. That’s computational indeterminacy.

Obviously, a computer is more than just one gate. It’s many many gates. As you add more and more gates into the system, the indeterminacy is going to multiply and multiply. Just one gate can give you OR or AND, but when you put enough gates together (like there are in a modern computer) the indeterminacy is only going to get more complex.

**_What is it good for?_**

This phenomenon was discovered by Ralph Slutz, the 20th century computational engineer. However, its importance for philosophy and cognitive science is only now becoming apparent. **Why is it so important?** This is something Jack Copeland focussed on in his talk. Return to our example of a gate that could be OR or AND. Imagine you have a computer, and it has two different tasks to perform. Suppose that, in order to perform task 1, it will need an OR gate. In order to perform task 2, it needs an AND gate. Here’s the crucial question: **will this computer need two different gates to do this?**

**No!** As we saw above, whether a gate computes AND or OR is a matter of interpretation, so in a sense, the gate performs both of them. Y**ou only need one gate, and it can compute both.** All you need is two different systems that interpret the gate (these are what Jack Copeland calls ‘probes’). One probe interprets the gate as an AND gate, the other interprets it as an OR gate. So you only need one gate, and it can do the job of an AND gate, and an OR gate. Your computer can perform both tasks with only one gate. You get two gates for the price of one. Gates, it turns out, are great at multi-tasking.

**This could be especially useful in reducing redundancy in machine design.** Rather than using huge numbers of gates, each of which doing one job, we might hope to get each one to multi-task. Ultimately, we might hope to end up with **one large central system, with many gates all doing their thing**. We might then hope that there could be multiple probes, each of which interprets this system in a different way. Maybe one of them interprets the system in a way that is relevant for one task, another which is relevant for another, and so on. All of the tasks are performed by just one system, but it gets put to many different uses, by being interpreted in different ways by different probes. **It’s like getting many many different computations, all for the price of one!**

**_Philosophical questions:_**

Let’s leave machine design to one side, and concentrate on the philosophical implications of all of this. Oron Shagrir takes the indeterminacy one step further. Above, we used the example of just two possible states (pulse and non-pulse) and two interpretations (TRUE/1 and FALSE/0). But of course, in reality, there are many states. Imagine you have three possible states. For example, suppose that instead of taking just a pulse or a non-pulse as inputs, the gate can take 3 volts, 6 volts, and 10 volts as inputs. Now we have three possible input states. And you can map these to TRUE/1 and FALSE/0 in different ways too. You could interpret 3 volts as TRUE/1, and then interpret 6 and 10 volts each as FALSE/0. Or you could go the other way, and interpret 3 or 6 volts as TRUE/1, and 10 volts as FALSE/0 (obviously there will be many more possible interpretations).

Now, by making things just a tiny bit more complicated in this way (by scrapping just pulse and non-pulse and replacing them with 3, 6 and 10 voltages, and mapping them in different ways to TRUE/1, and FALSE/0), the number of computations that a gate can compute will go up. One gate could not just do OR or AND, but also other ones as well, such as XOR (XOR is ‘exclusive-or’: it will output ‘TRUE’ only in the case that _exactly one_ of the inputs is ‘TRUE’).

Here we encounter a problem. As we have seen, **one physical system can perform many different computations at once (in our first example, the really simple gate can compute OR and AND at the same time). But which physical systems can be correctly interpreted in these ways? Where does this end?** Let’s take the example of a rock. The rock is made up of atoms and molecules, each of which is engaged in complex physical activities. Could we interpret these complex physical interactions as performing computations, in the same way that we interpreted our really simple gate above as performing AND and OR? Could we end up attributing computations to a rock? Surely that feels wrong. **Rocks are not computers!**

These are called ‘triviality results’, first introduced by philosophers such as Hilary Putnam. These are cases where you basically end up saying that everything is a computer. But we don’t want that. **We want to say that laptops, smartphones (maybe brains) compute things. Rocks, sticks, stones, and sealing wax do not. We need to draw the line in the right place.**

Oron’s own proposal is that we use **semantics** to solve this problem. The really computational states are the ones that have **semantic content**. That is, the ones that carry **meanings**, that **represent things** in the world. **Contrary to what a lot of philosophers have thought, computation may be a deeply semantic phenomenon...**

# Day 7: **"We know everything, but we understand nothing!"**

Scribe: [_Melvin Wevers_](/fellows#wevers "Melvin Wevers")

Chair: [_Jakub Growiec_](/fellows#growiec "Jakub Growiec")

The [Fellows](/fellows "Fellows") and [Mentors](/mentors "Mentors") of [ICA4](/about/concept "Concept") embarked on their last **scientific trip to** [**The École Normale Supérieure of Paris**](https://www.ens.psl.eu/en "ENS"). Two back to back scientific sessions were held in the morning, both of which described **the current issues related to Artificial Intelligence and offers future perspectives**. In the first talk, [**_Marc Mézard_**](/mentors#mezard "Marc Mezard"), the Director of the École Normale Supérieure (ENS) in Paris, gave his perspective on **recent progress and future challenges in AI**. Mézard is a theoretical physicist with a personal interest in the development of a theoretical framework to explain how AI works, and more specifically how Deep Neural Networks operate. Huge innovations have been made in the predictive power of neural networks. Still, many of the conceptual foundations have been around since the 1980s. By describing the lineage of the technology, Mézard was able to convincingly argue that the lack of a theoretical grounding for these networks. How do they work? We know everything about these networks, but we understand nothing, the speaker provocatively posed.

Notwithstanding the impressive technological innovations in Deep Neural Networks, **Mézard raises three main issues**:

* The **training of the networks** still requires vast amounts of data, which is unpractical and a sign that the networks do not mimic the human brain. Humans can already learn and generalize after being exposed to a small set of training material.
* There is still **a clear lack of understanding of what is going on in neural networks**. The learning mechanism in networks is poorly understood. In other words, there is no way to explain how the machine makes decisions.
* **Neural networks are not able to generate representations of the world**. Neural networks are extremely adept at making predictions, but they are not able to generate representations of the world. All in, we are still very far from reaching General AI.

Mézard stressed that we need **a better understanding of architecture, algorithms, and data structure,** which can improve the explainability of AI. In addition, there is a need for **a global set of ethical rules and regulations**. We need control mechanisms and **a global vision of the possible impacts of AI on our societies**.

The economist [**_Philippe Aghion_**](/mentors#aghion "Philippe Aghion"), professor at College de France and London School of Economics, expanded on **ways to stimulate research into AI and the development of AI in industry, while also emphasizing the need for regulation**. Aghion drew from his expertise on economic growth and drew parallels to the role of AI in society and global economies.

AI has already had a considerable impact on society, for example, through **the impact of automation on the labour force**. While AI has been instrumental in increasing productivity, **economic growth has declined since the mid-2000s**. Aghion's major concern is focused on the **formation of large companies which boosted growth but also inhibited innovation**. Much of the innovation related to AI is currently concentrated in such companies. To give AI more potential, we need to rethink ways to stimulate innovation while also making it sustainable. One crucial step is to re-calibrate the relationship between companies, institutions, and civic society. Rethinking funding strategies while also thinking of governmental regulatory measures and increasing the civic engagement with these companies is vital to achieving **sustainable growth** for companies developed AI.

Both talks showed **the complicated interplay between research objectives related to AI and the societal and economic embedding of the technology**. Current research is devoting more attention to the **challenges** that Mézard raised, for example, work on Explainable AI. However, **public awareness of the limitations of neural networks and the research challenges is still lagging**. Increasing this **awareness** might help to balance polarized views on AI that often oscillate between dystopian and utopian perspectives. **An interdisciplinary project**, offered through [**the 4th Intercontinental Academia**](/about/concept "Concept"), is of utmost importance in **shaping how scholars should communicate the current state of AI and its challenges to a broader audience**.

# The Final Day: **where we go from here...**

Scribe: [_Mike Livermore_](/fellows#livermore "Mike Livermore")

Chair: [_Alex Cayco Gajic_](/fellows#cayco-gajic "Alex Cayco Gajic")

**The top-down and bottom-up in visual processing**

Presented by [**_Shimon Ullman_**](/mentors#ullman "Shimon Ullman")

A major question in visual processing is **how humans extract information from complex scenes**. Images often tell us a story! Extracting such a narrative from a complex scene is a sophisticated task. There is a great deal of **cultural and situational knowledge** that serves as the background for directing attentional and visual processing resources.

Deep neural networks have made substantial progress in certain types of visual processing tasks. But there are massive data requirements. **Merely identifying objects and characteristics is not enough to account for relationships between objects**. Humans are very fast at identifying the important features in an image that relate to structure and narrative. There is an interesting set of psychophysics studies that examine how people extract information based on the time of exposure to an image. What we learn from these studies is that **there is a substantial amount of cross-talk between visual and cognitive systems**. It’s not the case that the visual system just analyses a scene and then sends the processed information upstream. Rather some visual information goes up to the cognitive centres, and that information is used to direct queries carried out by the visual system, which sends information up to the cognitive centres in an iterative process.

**Can we model this process partially in an artificial system as an “unfolded RNN” that involves both bottom-up and top-down layers?** In such a system, there are both **symbolic and embedded representations**. The inputs for the models are the images themselves and a set of instructions. The instructions are queries structured as vectors coded over objects and properties. The algorithm returns a correct answer when it pulls the queried information from the image. We can interpret what’s happening here symbolically as a program or sequential set of instructions for extracting information from an image.

A major challenge is a combinatorial generalisation. The same structure can be instantiated in many ways (_compare_ “A brown dog chasing a terrified kitten” _with_ “A large cat chasing a furry squirrel”). One way to test this is to leave out object/property pairs from the training set and test these. The bottom-up/top-down systems do well with the left-out data, but the bottom-up only system performs poorly on the left-out data.

There is a broader set of questions on modelling and understanding. Humans have a very high-level understanding of a concept like “drinking” that is very hard to imagine arising from a purely bottom-up model. More data, even massive amounts of data, might not be sufficient without some higher-order structure.

Upon completion of the presentation, [ICA4 Fellows](/fellows "Fellows") then asked questions related to how images are embedded in actions and within cultural contexts, the relationship between ontologies and bottom-up networks, and visual processing in non-humans and how that informs our thinking on the role of abstract reasoning in visual processing.

**The role of the robot in society**

Presented by [**_Zaven Paré_**](/mentors#pare "Zaven Pare")

**Art** can tell us something about society and technology. **Once robots are embedded in contexts, they take on new characteristics** – even if they are manufactured to be the “same,” they are changed by their environments. Different robots serve different functions in different societies. **An important potential role for robots is to respond to isolation**. Robots in deep sea and space are examples of how these systems can generate human-artificial interaction in conditions of isolation. [**The Gatebox product**](https://www.gatebox.ai/en/ "Gatebox") is a provocative example of **an existing robot that is addressed to isolation in a contemporary urban environment**.

The fellows then discussed questions related to the importance of isolation, what it means for a robot to offer companionship and the social meaning of different anthropic forms being projected onto robots.

Upon completion of the day's scientific sessions, the Fellows then discussed their follow-up tasks and recurrent meetings. The first session of ICA4 in Paris was then wrapped up, and the key takeaways, as well as questions to explore further, were noted and presented by the Fellows, while also reflecting on the previous days.

The intellectually intense series of events was then concluded with some cocktails at the [IEA de Paris](https://www.paris-iea.fr/en/ "Paris IEA"), marking the very beginning of a series of scientific adventures which are yet to come, as our Fellows continue to collectively explore some seemingly never-ending questions through combining various perspectives on [**Intelligence and Artificial Intelligence**](/about/ica4 "Concept"), ultimately discovering and shaping how such complex matters should, and will be, embedded within societies...